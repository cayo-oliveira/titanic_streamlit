
# Guia de Aula ‚Äî Kaggle Chatbot MVP (Python + Streamlit)

## 1) Objetivo da aula

Montar um **MVP** de app com **Python + Streamlit** capaz de:

- **Conversar** sobre o tema escolhido (chat ‚Äúguiado‚Äù pelas features);
- **Treinar** um modelo simples (regress√£o **log√≠stica** ou **linear**);
- **Responder perguntas** do usu√°rio com base em **m√©tricas** e **import√¢ncias de vari√°veis**;
- Estar pronto para **deploy** (Streamlit Cloud/Render) com **docs organizadas** (PM Canvas, arquitetura, dados, LGPD, governan√ßa DAMA, testes).

---

## 2) Estrutura de reposit√≥rio (padr√£o para todos os temas)

> Um **√∫nico reposit√≥rio** com pastas ‚Äúde responsabilidade‚Äù (parecendo **camadas**), mantendo a simplicidade do MVP.

```
kaggle-chatbot-mvp/
‚îú‚îÄ app/                         # "Front" Streamlit e rotas de UI
‚îÇ  ‚îú‚îÄ pages/                    # P√°ginas extras do Streamlit (opcional)
‚îÇ  ‚îî‚îÄ main_app.py               # App principal (Streamlit)
‚îÇ
‚îú‚îÄ core/                        # "Back" de regras do dom√≠nio (sem UI)
‚îÇ  ‚îú‚îÄ data/                     # Fun√ß√µes de carga/valida√ß√£o de dados
‚îÇ  ‚îÇ  ‚îú‚îÄ io.py                  # ler_csv, salvar_csv, baixar_zip (se preciso)
‚îÇ  ‚îÇ  ‚îî‚îÄ schema.py              # checagens simples (tipos, colunas obrigat√≥rias)
‚îÇ  ‚îú‚îÄ features/                 # Engenharia de atributos
‚îÇ  ‚îÇ  ‚îî‚îÄ preprocess.py          # pipelines: imputar, one-hot, escala
‚îÇ  ‚îú‚îÄ models/                   # Treino e predi√ß√£o
‚îÇ  ‚îÇ  ‚îú‚îÄ train.py               # treinar_regressao, treinar_classificacao
‚îÇ  ‚îÇ  ‚îî‚îÄ predict.py             # carregar_modelo, prever, explicar_resultado
‚îÇ  ‚îú‚îÄ explain/                  # Explicabilidade simples
‚îÇ  ‚îÇ  ‚îî‚îÄ coefficients.py        # coeficientes, odds ratio, import√¢ncia relativa
‚îÇ  ‚îî‚îÄ chatbot/                  # Regras do "chat" por tema (sem LLM)
‚îÇ     ‚îî‚îÄ rules.py               # respostas guiadas (FAQ + m√©tricas/coeficientes)
‚îÇ
‚îú‚îÄ configs/
‚îÇ  ‚îú‚îÄ settings.example.toml     # vari√°veis de config (tema, alvo, etc)
‚îÇ  ‚îî‚îÄ logging.conf              # logging b√°sico
‚îÇ
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                      # CSVs brutos (n√£o comitar dados sens√≠veis)
‚îÇ  ‚îú‚îÄ processed/                # CSVs tratados
‚îÇ  ‚îî‚îÄ models/                   # .pkl dos modelos (ok no MVP)
‚îÇ
‚îú‚îÄ notebooks/                   # Explora√ß√£o inicial (EDA)
‚îÇ  ‚îî‚îÄ 01_eda_titanic.ipynb
‚îÇ
‚îú‚îÄ tests/
‚îÇ  ‚îú‚îÄ test_data_io.py
‚îÇ  ‚îú‚îÄ test_preprocess.py
‚îÇ  ‚îî‚îÄ test_models.py
‚îÇ
‚îú‚îÄ docs/                        # "Docusaurus-like" em Markdown
‚îÇ  ‚îú‚îÄ README.md                 # documenta√ß√£o portal (√≠ndice)
‚îÇ  ‚îú‚îÄ pmc.md                    # Project Model Canvas (foto + campos)
‚îÇ  ‚îú‚îÄ architecture.md           # Arquitetura de Software (com .drawio)
‚îÇ  ‚îú‚îÄ data_model.md             # DER/MER e dicion√°rio de dados
‚îÇ  ‚îú‚îÄ governance_lgpd.md        # DAMA + LGPD aplicados
‚îÇ  ‚îú‚îÄ testing.md                # Estrat√©gia de testes
‚îÇ  ‚îú‚îÄ deployment.md             # Deploy (Streamlit Cloud/Render)
‚îÇ  ‚îî‚îÄ theme_guides/
‚îÇ     ‚îú‚îÄ titanic.md
‚îÇ     ‚îú‚îÄ ecommerce.md
‚îÇ     ‚îî‚îÄ ...
‚îÇ
‚îú‚îÄ .gitignore
‚îú‚îÄ requirements.txt
‚îú‚îÄ runtime.txt                  # (opcional) travar vers√£o do Python p/ deploy
‚îî‚îÄ Makefile                     # (opcional) atalhos: make run, make test
```

**Por que assim?**

- `app/` √© o ‚Äúfront‚Äù Streamlit; `core/` √© a ‚Äúl√≥gica de neg√≥cio‚Äù; `docs/` centraliza documenta√ß√£o (estilo Docusaurus em Markdown), incluindo **PMC, arquitetura, dados, LGPD, governan√ßa**.
- `data/` guarda datasets e modelos (no MVP; futuramente usar storage externo).
- `tests/` d√° o h√°bito de **testes unit√°rios e de integra√ß√£o**.

---

## 3) Template de c√≥digo (tema Titanic como exemplo)

### 3.1 `app/main_app.py` (UI do Streamlit)

```python
import streamlit as st
import pandas as pd
from core.data.io import read_csv_smart
from core.features.preprocess import make_preprocess_pipeline
from core.models.train import train_classifier, train_regressor
from core.models.predict import evaluate_classifier, evaluate_regressor
from core.explain.coefficients import extract_logit_importances, extract_linear_importances
from core.chatbot.rules import answer_from_metrics

st.set_page_config(page_title="Chatbot Kaggle MVP", layout="wide")

st.title("üß™ Kaggle Chatbot MVP ‚Äî Tema Titanic")

st.sidebar.header("Configura√ß√µes")
task = st.sidebar.selectbox("Tarefa", ["Classifica√ß√£o (Survived)", "Regress√£o (Fare)"])
test_size = st.sidebar.slider("Tamanho do teste", 0.1, 0.4, 0.2, 0.05)

uploaded = st.file_uploader("Envie o CSV do Titanic (train.csv)", type=["csv"])
question = st.text_input("Pergunte algo ao chatbot (ex.: 'Quais vari√°veis mais importam?')")

if uploaded:
    df = read_csv_smart(uploaded)
    st.write("Pr√©via dos dados", df.head())

    # Defini√ß√µes simples de colunas
    drop_cols = [c for c in ["PassengerId","Name","Ticket","Cabin"] if c in df.columns]
    df = df.drop(columns=drop_cols)

    if task.startswith("Classifica√ß√£o"):
        target = "Survived"
        y = df[target]
        X = df.drop(columns=[target])

        pre = make_preprocess_pipeline(X)
        model, X_test, y_test = train_classifier(X, y, pre, test_size=test_size)

        metrics, cm = evaluate_classifier(model, X_test, y_test)
        st.subheader("üìà M√©tricas (Classifica√ß√£o)")
        st.write(metrics)
        st.write("Matriz de Confus√£o", cm)

        importances = extract_logit_importances(model, X.columns, pre)
        st.subheader("üîé Import√¢ncias (Logistic Coef / Odds Ratio)")
        st.dataframe(importances.head(20))

        if question:
            st.info(answer_from_metrics(question, task, metrics, importances))

    else:
        target = "Fare"
        y = df[target]
        X = df.drop(columns=[target])

        pre = make_preprocess_pipeline(X)
        model, X_test, y_test = train_regressor(X, y, pre, test_size=test_size)

        metrics = evaluate_regressor(model, X_test, y_test)
        st.subheader("üìà M√©tricas (Regress√£o)")
        st.write(metrics)

        importances = extract_linear_importances(model, X.columns, pre)
        st.subheader("üîé Import√¢ncias (Coeficientes normalizados)")
        st.dataframe(importances.head(20))

        if question:
            st.info(answer_from_metrics(question, task, metrics, importances))
```

---

### 3.2 `core/data/io.py`

```python
import pandas as pd

def read_csv_smart(file_or_path):
    # tenta detectar separador automaticamente
    return pd.read_csv(file_or_path, sep=None, engine="python")
```

---

### 3.3 `core/features/preprocess.py`

```python
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

def infer_cols(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in df.columns if c not in num_cols]
    return num_cols, cat_cols

def make_preprocess_pipeline(X_df):
    num_cols, cat_cols = infer_cols(X_df)

    num_pipe = SimpleImputer(strategy="median")
    cat_pipe = Pipeline(steps=[
        ("impute", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    pre = ColumnTransformer([
        ("num", num_pipe, num_cols),
        ("cat", cat_pipe, cat_cols)
    ])

    # scaler ap√≥s o ColumnTransformer (para regress√£o/coefs est√°veis)
    return Pipeline([("pre", pre), ("scaler", StandardScaler(with_mean=False))])
```

> Obs.: note o `from sklearn.pipeline import Pipeline` no topo.

---

### 3.4 `core/models/train.py`

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.pipeline import Pipeline

def split(X, y, test_size=0.2, random_state=42):
    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=None)

def train_classifier(X, y, pre, test_size=0.2):
    X_train, X_test, y_train, y_test = split(X, y, test_size=test_size)
    clf = LogisticRegression(max_iter=1000)
    model = Pipeline([("pre", pre), ("clf", clf)])
    model.fit(X_train, y_train)
    return model, X_test, y_test

def train_regressor(X, y, pre, test_size=0.2):
    X_train, X_test, y_train, y_test = split(X, y, test_size=test_size)
    reg = LinearRegression()
    model = Pipeline([("pre", pre), ("reg", reg)])
    model.fit(X_train, y_train)
    return model, X_test, y_test
```

---

### 3.5 `core/models/predict.py`

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error
import numpy as np

def evaluate_classifier(model, X_test, y_test):
    y_pred = model.predict(X_test)
    metrics = {
        "accuracy": float(accuracy_score(y_test, y_pred)),
        "precision": float(precision_score(y_test, y_pred, zero_division=0)),
        "recall": float(recall_score(y_test, y_pred, zero_division=0)),
        "f1": float(f1_score(y_test, y_pred, zero_division=0)),
    }
    cm = confusion_matrix(y_test, y_pred).tolist()
    return metrics, cm

def evaluate_regressor(model, X_test, y_test):
    y_pred = model.predict(X_test)
    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
    return {"rmse": rmse}
```

---

### 3.6 `core/explain/coefficients.py`

```python
import numpy as np
import pandas as pd

def _feature_names_from_preprocess(pre, original_cols):
    # tenta extrair nomes ap√≥s OneHot
    pre_step = pre.named_steps["pre"]
    num_cols = pre_step.transformers_[0][2]
    cat_cols = pre_step.transformers_[1][2]
    # num: nomes originais
    out = list(num_cols)
    # cat: pega categories_ do OneHot
    ohe = pre_step.named_transformers_["cat"].named_steps["onehot"]
    cat_feature_names = ohe.get_feature_names_out(cat_cols).tolist()
    return out + cat_feature_names

def extract_logit_importances(model_pipe, original_cols, pre):
    # modelo final: Pipeline(pre ‚Üí clf)
    clf = model_pipe.named_steps["clf"]
    feature_names = _feature_names_from_preprocess(model_pipe.named_steps["pre"], original_cols)
    coefs = clf.coef_.ravel()
    odds = np.exp(coefs)
    df = pd.DataFrame({"feature": feature_names, "coef": coefs, "odds_ratio": odds})
    df["abs_coef"] = df["coef"].abs()
    return df.sort_values("abs_coef", ascending=False).drop(columns="abs_coef")

def extract_linear_importances(model_pipe, original_cols, pre):
    reg = model_pipe.named_steps["reg"]
    feature_names = _feature_names_from_preprocess(model_pipe.named_steps["pre"], original_cols)
    coefs = reg.coef_.ravel()
    df = pd.DataFrame({"feature": feature_names, "coef": coefs, "abs_coef": np.abs(coefs)})
    return df.sort_values("abs_coef", ascending=False)
```

---

### 3.7 `core/chatbot/rules.py` (chat ‚Äúregrado‚Äù sem LLM)

```python
def answer_from_metrics(question: str, task: str, metrics_df_or_dict, importances_df):
    q = (question or "").lower()

    if "importan" in q or "import√¢n" in q or "vari√°ve" in q or "features" in q:
        top = importances_df.head(5)[["feature"]].to_dict("records")
        top_str = ", ".join([t["feature"] for t in top])
        return f"As vari√°veis mais influentes s√£o: {top_str}. (Baseado em coeficientes/odds ratio)"

    if "m√©tric" in q or "score" in q or "acur" in q or "rmse" in q:
        return f"M√©tricas da tarefa {task}: {metrics_df_or_dict}"

    if "como foi treinado" in q or "pipeline" in q:
        return "O pipeline aplica imputa√ß√£o, one-hot e padroniza√ß√£o; depois treina Logistic Regression (class.) ou Linear Regression (regr.)."

    if "privacid" in q or "lgpd" in q:
        return "No MVP, evitamos dados sens√≠veis, anonimiza√ß√£o por padr√£o e n√£o persistimos dados pessoais. Para produ√ß√£o: consentimento expresso, minimiza√ß√£o e auditoria."

    return "Posso falar sobre vari√°veis importantes, m√©tricas do modelo e como o pipeline funciona. Pergunte algo como 'Quais vari√°veis mais importam?'."
```

---

## 4) Documenta√ß√£o estilo ‚ÄúDocusaurus‚Äù (em Markdown)

### 4.1 `docs/README.md` (portal)

```markdown
# Kaggle Chatbot MVP

MVP educacional para responder perguntas sobre um tema do Kaggle via Streamlit, com treino de modelo (regress√£o log√≠stica ou linear) e documenta√ß√£o integrada (PMC, arquitetura, dados, LGPD/DAMA, testes e deploy).

## Como rodar
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
streamlit run app/main_app.py
```

## Estrutura
- `app/` UI (Streamlit)
- `core/` regras de neg√≥cio (dados, features, modelos, chatbot, explicabilidade)
- `docs/` documenta√ß√£o do projeto (PMC, arquitetura, dados, governan√ßa, testes, deploy)
- `data/` dados e modelos (MVP)
- `tests/` testes unit./integra√ß√£o

## Deploy r√°pido
Veja `docs/deployment.md`.
```
```

### 4.2 `docs/pmc.md` (Project Model Canvas)

- Adicione a **imagem do PMC preenchido** pelo grupo (exportada do Miro/Whimsical/Excalidraw).
- Inclua os blocos adaptados a software: **Objetivo, Justificativa, Produto, Requisitos (inclui LGPD+DAMA), P√∫blico-alvo, Equipe, Riscos, Cronograma, Custos (esfor√ßo), Recursos, Stakeholders**.

### 4.3 `docs/architecture.md` (Arquitetura de Software)

- Inclua um **.drawio** com:
  - M√≥dulos: `app (Streamlit)` ‚Üí `core (data, features, models, chatbot)` ‚Üí `data/ (arquivos)`.
  - Fluxos: **Upload CSV ‚Üí Pr√©-processar ‚Üí Treinar ‚Üí M√©tricas ‚Üí Resposta do Chat**.
  - Setas e anota√ß√µes de **camadas** (UI, aplica√ß√£o, dados).
- Exporte uma imagem PNG e embuta:
  ```markdown
  ![Arquitetura](./images/architecture.png)
  ```

### 4.4 `docs/data_model.md` (Arquitetura/Modelagem de Dados)

- **DER simples** (ex.: Tabela ‚Äúpassengers‚Äù no Titanic) com campos e tipos.
- **Dicion√°rio de dados** (nome, tipo, descri√ß√£o, dom√≠nio).

### 4.5 `docs/governance_lgpd.md` (Governan√ßa DAMA + LGPD)

- **DAMA**: qualidade (valida√ß√£o de entrada), metadados (README do dataset), seguran√ßa (n√£o versionar dados sens√≠veis), ciclo de vida (limpeza da pasta `data/`).
- **LGPD**: minimiza√ß√£o (n√£o coletar al√©m do necess√°rio), consentimento (documentar como seria), anonimiza√ß√£o, auditoria (logs b√°sicos).

### 4.6 `docs/testing.md`

- Testes unit√°rios: `read_csv_smart`, `make_preprocess_pipeline`, `train_classifier/regressor`.
- Testes de integra√ß√£o: fluxo **‚ÄúCSV ‚Üí m√©tricas‚Äù**.

### 4.7 `docs/deployment.md` (Deploy simples)

- **Streamlit Cloud**:
  1. Repo p√∫blico no GitHub
  2. App: `app/main_app.py`
  3. Python 3.11 (por ex.), `requirements.txt`
  4. `data/` vazia (ou exemplo pequeno)
- **Render.com** (opcional): usar `gunicorn` + `streamlit` com *start command*.

---

## 5) `requirements.txt` sugerido

```txt
pandas
numpy
scikit-learn
streamlit
```

> Se usar gr√°ficos, adicione `plotly`.

---

## 6) Testes (exemplos m√≠nimos)

### `tests/test_data_io.py`

```python
from io import StringIO
from core.data.io import read_csv_smart

def test_read_csv_smart():
    csv = "A,B\n1,2\n3,4\n"
    df = read_csv_smart(StringIO(csv))
    assert df.shape == (2,2)
```

### `tests/test_preprocess.py`

```python
import pandas as pd
from core.features.preprocess import make_preprocess_pipeline

def test_make_preprocess():
    X = pd.DataFrame({"num":[1,2,3], "cat":["a","b","a"]})
    pre = make_preprocess_pipeline(X)
    Xt = pre.fit_transform(X)
    assert Xt.shape[0] == 3
```
